{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty(\"spark.executor.memory\", \"4g\")\n",
    "sc = SparkContext('local[1]')\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '41895'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.app.id', 'local-1558708349624'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '10.0.2.15'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[1]'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a table from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|line_number|                text|label|\n",
      "+-----------+--------------------+-----+\n",
      "|          0|awww that bummer ...|    0|\n",
      "|          1|is upset that he ...|    0|\n",
      "|          2|dived many times ...|    0|\n",
      "|          3|my whole body fee...|    0|\n",
      "|          4|no it not behavin...|    0|\n",
      "|          5|  not the whole crew|    0|\n",
      "|          6|            need hug|    0|\n",
      "|          7|hey long time no ...|    0|\n",
      "|          8|nope they did not...|    0|\n",
      "|          9|        que me muera|    0|\n",
      "+-----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql('use project')\n",
    "df = hc.sql('select * from tweet_orc where line_number is not null')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line_number: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|line_number|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "|          5|\n",
      "|          6|\n",
      "|          7|\n",
      "|          8|\n",
      "|          9|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"line_number\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.dropna()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+--------------------+--------------------+--------------------+-----+\n",
      "|line_number|                text|label|               words|                  tf|            features|class|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+--------------------+-----+\n",
      "|          0|awww that bummer ...|    0|[awww, that, bumm...|(65536,[8436,8847...|(65536,[8436,8847...|  0.0|\n",
      "|          0|awww that bummer ...|    0|[awww, that, bumm...|(65536,[8436,8847...|(65536,[8436,8847...|  0.0|\n",
      "|          1|is upset that he ...|    0|[is, upset, that,...|(65536,[1444,2071...|(65536,[1444,2071...|  0.0|\n",
      "|          1|is upset that he ...|    0|[is, upset, that,...|(65536,[1444,2071...|(65536,[1444,2071...|  0.0|\n",
      "|          2|dived many times ...|    0|[dived, many, tim...|(65536,[2548,2888...|(65536,[2548,2888...|  0.0|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "#minDocFreq: remove sparse terms\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) \n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"class\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "test_df = pipelineFit.transform(test_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valication accuracy:  0.7975956284153005\n",
      "test accuracy:  0.80287222676136\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "\n",
    "predictions = lrModel.transform(val_df)\n",
    "# evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "# evaluator.evaluate(predictions)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "print(\"valication accuracy: \", accuracy)\n",
    "predictions_test = lrModel.transform(test_df)\n",
    "accuracy_test = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())\n",
    "print(\"test accuracy: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with CountVectorizer and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|line_number|                text|label|               words|                  cv|            features|class|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|          0|awww that bummer ...|    0|[awww, that, bumm...|(16384,[0,3,5,10,...|(16384,[0,3,5,10,...|  0.0|[3.68135290712708...|[0.97543001663847...|       0.0|\n",
      "|          0|awww that bummer ...|    0|[awww, that, bumm...|(16384,[0,3,5,10,...|(16384,[0,3,5,10,...|  0.0|[3.68135290712708...|[0.97543001663847...|       0.0|\n",
      "|          1|is upset that he ...|    0|[is, upset, that,...|(16384,[3,4,6,7,1...|(16384,[3,4,6,7,1...|  0.0|[4.06736991567062...|[0.98316587735562...|       0.0|\n",
      "|          1|is upset that he ...|    0|[is, upset, that,...|(16384,[3,4,6,7,1...|(16384,[3,4,6,7,1...|  0.0|[4.06736991567062...|[0.98316587735562...|       0.0|\n",
      "|          2|dived many times ...|    0|[dived, many, tim...|(16384,[0,1,9,10,...|(16384,[0,1,9,10,...|  0.0|[-0.4221955820047...|[0.39599148619943...|       1.0|\n",
      "+-----------+--------------------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "#minDocFreq: remove sparse terms\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) \n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"class\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "#train_df = pipelineFit.transform(train_set)\n",
    "#val_df = pipelineFit.transform(val_set)\n",
    "#test_df = pipelineFit.transform(test_set)\n",
    "#train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy Score: 0.7990\n",
      "Test Accuracy Score: 0.8027\n"
     ]
    }
   ],
   "source": [
    "predictions_val = pipelineFit.transform(val_set)\n",
    "accuracy = predictions_val.filter(predictions_val.label == predictions_val.prediction).count() / float(val_set.count())\n",
    "print(\"Validation Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "#roc_auc = evaluator.evaluate(predictions)\n",
    "#print \"ROC-AUC: {0:.4f}\".format(roc_auc)\n",
    "\n",
    "predictions_t = pipelineFit.transform(test_set)\n",
    "accuracy_test = predictions_t.filter(predictions_t.label == predictions_t.prediction).count() / float(test_set.count())\n",
    "print(\"Test Accuracy Score: {0:.4f}\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression with N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stephen/.local/lib/python3.6/site-packages/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'HashingTF' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "ngram = NGram(n=1, inputCol=\"words\", outputCol=\"n_gram\")\n",
    "hashtf = HashingTF(numFeatures=2**16,inputCol=\"n_gram\", outputCol=\"tf\")\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) \n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"class\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram, hashtf, idf, label_stringIdx, lr])\n",
    "pipelineFit = pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy Score: 0.7976\n",
      "Test Accuracy Score: 0.8029\n"
     ]
    }
   ],
   "source": [
    "predictions_val = pipelineFit.transform(val_set)\n",
    "accuracy = predictions_val.filter(predictions_val.label == predictions_val.prediction).count() / float(val_set.count())\n",
    "print(\"Validation Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "\n",
    "predictions_t = pipelineFit.transform(test_set)\n",
    "accuracy_test = predictions_t.filter(predictions_t.label == predictions_t.prediction).count() / float(test_set.count())\n",
    "print(\"Test Accuracy Score: {0:.4f}\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
